---
title: "Lab07"
author: "VY"
date: "`r Sys.Date()`"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```
```{r set up packages, cache = TRUE}
library(dplyr)
library(data.table)
library(ggplot2)
library(tidytext)
library(tidyverse)
library(dtplyr)
library(knitr)
library(forcats)
library(utils)
library(xml2)
library(rvest)
library(httr)
```

# Question 1: How many sars-cov-2 papers?
```{r download website and extract data, cache=TRUE}
# Downloading the website
website <- xml2::read_html("https://pubmed.ncbi.nlm.nih.gov/?term=sars-cov-2")

# Finding the counts
counts <- xml2::xml_find_first(website, "/html/body/main/div[9]/div[2]/div[2]/div[1]/div[1]/span")

# Turning it into text
counts <- as.character(counts)

# Extracting the data using regex
stringr::str_extract(counts, "[0-9,.]+")
```
# Question 2: Academic publications on COVID19 and Hawaii
```{r query, cache=TRUE}
library(httr)
query_ids <- GET(
  url   = "https://eutils.ncbi.nlm.nih.gov/entrez/eutils/esearch.fcgi",
  query = list(
    db = "pubmed",
    term = "covid19 hawaii",
    retmax = 1000
  )
)

# Extracting the content of the response of GET
ids <- httr::content(query_ids)

```

# Question 3: Get details about the articles
```{r article details, cache=TRUE}
# Turn the result into a character vector
# ids <- as.character(ids)

# Find all the ids 
ids <- stringr::str_extract_all(ids, "<Id>[[:digit:]]+</Id>")[[1]]

# Remove all the leading and trailing <Id> </Id>. Make use of "|"
ids <- stringr::str_remove_all(ids, "</?Id>")

#publications <- GET(
 # url   = "https://eutils.ncbi.nlm.nih.gov/entrez/eutils/efetch.fcgi",
  #query = list(
   # db = "pubmed",
    #id = ,
    #retmax = 1000,
    #rettype = "abstract"
    #)
#)

# Turning the output into character vector
#publications <- httr::content(publications)
#publications_txt <- as.character(publications) 

```



